{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction - Obligatorisk Opgave\n",
    "## Machine Learning E2025\n",
    "\n",
    "**Formål**: Forudsig hvilke passagerer der overlevede Titanic-forliset baseret på passagerdata.\n",
    "\n",
    "**Dataset**: `titanic_800.csv` - 800 passagerer med 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer nødvendige biblioteker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Sæt random seed for reproducerbarhed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Getting to know the data and the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indlæs datasættet\n",
    "data = pd.read_csv('titanic_800.csv')\n",
    "\n",
    "# Vis de første rækker\n",
    "print(\"Første 10 rækker:\")\n",
    "print(data.head(10))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Generel information om datasættet\n",
    "print(\"\\nDataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Statistisk beskrivelse\n",
    "print(\"\\nStatistisk beskrivelse:\")\n",
    "print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spørgsmål A: Hvad slags ML problem er dette?\n",
    "\n",
    "**Svar**: Dette er et **binært klassifikationsproblem** med **supervised learning**.\n",
    "\n",
    "- **Supervised**: Vi har labeled data (Survived = 0 eller 1)\n",
    "- **Binær klassifikation**: Target variabel har 2 klasser (0 = død, 1 = overlevede)\n",
    "- **Mål**: Forudsig om en passager overlevede baseret på features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tæl features og labels\n",
    "total_kolonner = len(data.columns)\n",
    "print(f\"\\nTotalt antal kolonner: {total_kolonner}\")\n",
    "print(f\"Kolonne navne: {list(data.columns)}\")\n",
    "\n",
    "# Features (ekskl. target variable)\n",
    "feature_kolonner = [col for col in data.columns if col != 'Survived']\n",
    "print(f\"\\nAntal features (ekskl. Survived): {len(feature_kolonner)}\")\n",
    "print(f\"Feature navne: {feature_kolonner}\")\n",
    "\n",
    "# Target variable\n",
    "print(f\"\\nY-data (label) består af: 1 kolonne ('Survived')\")\n",
    "print(f\"Survived værdier: {data['Survived'].unique()}\")\n",
    "print(f\"Distribution:\")\n",
    "print(data['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Svar på Spørgsmål A:\n",
    "\n",
    "1. **ML problem type**: Binær klassifikation med supervised learning\n",
    "2. **Antal features**: 11 features (ekskl. Survived)\n",
    "3. **Antal Y-data features**: 1 (Survived kolonne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tjek for manglende data\n",
    "print(\"\\nManglende data pr. kolonne:\")\n",
    "missing_data = data.isnull().sum()\n",
    "missing_pct = (data.isnull().sum() / len(data)) * 100\n",
    "missing_df = pd.DataFrame({'Manglende': missing_data, 'Procent': missing_pct})\n",
    "print(missing_df[missing_df['Manglende'] > 0].sort_values('Manglende', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Cleaning the data\n",
    "\n",
    "### Strategi:\n",
    "1. Fjern irrelevante features (PassengerId, Name, Ticket, Cabin)\n",
    "2. Analyser korrelation mellem features og survival\n",
    "3. Håndter manglende data\n",
    "4. Konverter kategoriske variabler til numeriske\n",
    "5. Feature scaling\n",
    "6. Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopier data for at bevare originalen\n",
    "df = data.copy()\n",
    "\n",
    "# 1. Fjern irrelevante features\n",
    "print(\"\\n=== FEATURE REMOVAL ===\")\n",
    "print(\"\\nFjerner følgende features:\")\n",
    "print(\"- PassengerId: Unik ID, ingen forudsigelseskraft\")\n",
    "print(\"- Name: Tekstdata, for høj kardinalitet (mange unikke navne)\")\n",
    "print(\"- Ticket: Ticket nummer, ingen klar sammenhæng med survival\")\n",
    "print(\"- Cabin: For mange manglende værdier (687/800 = 86%)\")\n",
    "\n",
    "df_clean = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "print(f\"\\nFørste kolonner: {len(df.columns)}\")\n",
    "print(f\"Efter rensning: {len(df_clean.columns)}\")\n",
    "print(f\"Tilbageværende kolonner: {list(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser survival rate for forskellige features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('Survival Rate analyse pr. Feature', fontsize=16)\n",
    "\n",
    "# Pclass\n",
    "sns.countplot(data=df_clean, x='Pclass', hue='Survived', ax=axes[0,0])\n",
    "axes[0,0].set_title('Pclass vs Survival')\n",
    "axes[0,0].legend(['Død', 'Overlevede'])\n",
    "\n",
    "# Sex\n",
    "sns.countplot(data=df_clean, x='Sex', hue='Survived', ax=axes[0,1])\n",
    "axes[0,1].set_title('Sex vs Survival')\n",
    "axes[0,1].legend(['Død', 'Overlevede'])\n",
    "\n",
    "# Embarked\n",
    "sns.countplot(data=df_clean, x='Embarked', hue='Survived', ax=axes[0,2])\n",
    "axes[0,2].set_title('Embarked vs Survival')\n",
    "axes[0,2].legend(['Død', 'Overlevede'])\n",
    "\n",
    "# Age distribution\n",
    "df_clean[df_clean['Survived']==0]['Age'].hist(bins=30, ax=axes[1,0], alpha=0.5, label='Død', color='red')\n",
    "df_clean[df_clean['Survived']==1]['Age'].hist(bins=30, ax=axes[1,0], alpha=0.5, label='Overlevede', color='green')\n",
    "axes[1,0].set_title('Age Distribution')\n",
    "axes[1,0].set_xlabel('Age')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Fare distribution\n",
    "df_clean[df_clean['Survived']==0]['Fare'].hist(bins=30, ax=axes[1,1], alpha=0.5, label='Død', color='red')\n",
    "df_clean[df_clean['Survived']==1]['Fare'].hist(bins=30, ax=axes[1,1], alpha=0.5, label='Overlevede', color='green')\n",
    "axes[1,1].set_title('Fare Distribution')\n",
    "axes[1,1].set_xlabel('Fare')\n",
    "axes[1,1].legend()\n",
    "\n",
    "# SibSp\n",
    "sns.countplot(data=df_clean, x='SibSp', hue='Survived', ax=axes[1,2])\n",
    "axes[1,2].set_title('SibSp vs Survival')\n",
    "axes[1,2].legend(['Død', 'Overlevede'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print survival rates\n",
    "print(\"\\n=== SURVIVAL RATES ===\")\n",
    "print(f\"\\nOverall survival rate: {df_clean['Survived'].mean():.2%}\")\n",
    "print(f\"\\nSurvival rate by Pclass:\")\n",
    "print(df_clean.groupby('Pclass')['Survived'].mean())\n",
    "print(f\"\\nSurvival rate by Sex:\")\n",
    "print(df_clean.groupby('Sex')['Survived'].mean())\n",
    "print(f\"\\nSurvival rate by Embarked:\")\n",
    "print(df_clean.groupby('Embarked')['Survived'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Håndter manglende data\n",
    "print(\"\\n=== HÅNDTERING AF MANGLENDE DATA ===\")\n",
    "print(\"\\nManglende data før håndtering:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Age: Fyld med median (mere robust end mean ved outliers)\n",
    "median_age = df_clean['Age'].median()\n",
    "print(f\"\\nAge median: {median_age}\")\n",
    "df_clean['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "# Embarked: Fyld med mode (mest almindelige værdi)\n",
    "mode_embarked = df_clean['Embarked'].mode()[0]\n",
    "print(f\"Embarked mode: {mode_embarked}\")\n",
    "df_clean['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "\n",
    "# Fare: Fyld med median\n",
    "median_fare = df_clean['Fare'].median()\n",
    "print(f\"Fare median: {median_fare}\")\n",
    "df_clean['Fare'].fillna(median_fare, inplace=True)\n",
    "\n",
    "print(\"\\nManglende data efter håndtering:\")\n",
    "print(df_clean.isnull().sum())\n",
    "print(\"\\n✓ Alle manglende værdier er håndteret!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Konverter kategoriske variabler til numeriske\n",
    "print(\"\\n=== ENCODING AF KATEGORISKE VARIABLER ===\")\n",
    "\n",
    "# Sex: male=1, female=0 (kvinder havde højere survival rate)\n",
    "print(\"\\nSex encoding: female=0, male=1\")\n",
    "df_clean['Sex'] = df_clean['Sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "# Embarked: One-hot encoding eller ordinal?\n",
    "# Vi bruger one-hot encoding da der ikke er ordinal relationship\n",
    "print(\"\\nEmbarked encoding: One-hot (C, Q, S)\")\n",
    "df_clean = pd.get_dummies(df_clean, columns=['Embarked'], drop_first=True, dtype=int)\n",
    "\n",
    "print(\"\\nKolonner efter encoding:\")\n",
    "print(list(df_clean.columns))\n",
    "print(f\"\\nShape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split i features (X) og target (y)\n",
    "print(\"\\n=== SPLIT I X og y ===\")\n",
    "\n",
    "X = df_clean.drop('Survived', axis=1)\n",
    "y = df_clean['Survived']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nX columns: {list(X.columns)}\")\n",
    "print(f\"\\ny distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train/test split\n",
    "print(\"\\n=== TRAIN/TEST SPLIT ===\")\n",
    "\n",
    "# Vi bruger 80/20 split (640 train, 160 test)\n",
    "# Stratify sikrer samme distribution af Survived i train og test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTræningsdata: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test data: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSurvival distribution i træning:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nSurvival distribution i test:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n✓ Distribution er balanceret pga. stratify parameter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spørgsmål B: Train/Test split reasoning\n",
    "\n",
    "**Valg**: 80% træning (640 samples), 20% test (160 samples)\n",
    "\n",
    "**Begrundelse**:\n",
    "- 800 samples er relativt lille dataset\n",
    "- 80/20 giver nok træningsdata til at lære patterns\n",
    "- 160 test samples er tilstrækkeligt til pålidelig evaluering\n",
    "- Bruger `stratify=y` for at bevare class distribution (38% survived)\n",
    "- Alternative: 70/30 ville give mere test data men mindre træningsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature scaling\n",
    "print(\"\\n=== FEATURE SCALING ===\")\n",
    "print(\"\\nSkala før scaling:\")\n",
    "print(X_train.describe())\n",
    "\n",
    "# StandardScaler: (x - mean) / std\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Konverter tilbage til DataFrame for læsbarhed\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"\\nSkala efter scaling:\")\n",
    "print(X_train_scaled_df.describe())\n",
    "print(\"\\n✓ Alle features er nu på samme skala (mean≈0, std≈1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Choosing a model and doing training\n",
    "\n",
    "Vi vil træne **3 forskellige modeller** og sammenligne:\n",
    "1. **Random Forest** - Ensemble metode, robust, håndterer non-linearity\n",
    "2. **Logistic Regression** - Simpel baseline, god til lineære relationships\n",
    "3. **Neural Network (MLP)** - Kan lære komplekse patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Random Forest\n",
    "print(\"\\n=== RANDOM FOREST CLASSIFIER ===\")\n",
    "print(\"\\nBegrundelse:\")\n",
    "print(\"- Ensemble af beslutningstræer → reducerer overfitting\")\n",
    "print(\"- Håndterer non-lineære sammenhænge godt\")\n",
    "print(\"- Robust overfor outliers\")\n",
    "print(\"- Giver feature importance\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # 100 træer i skoven\n",
    "    max_depth=10,      # Max dybde for at undgå overfitting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"\\n✓ Random Forest trænet!\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualiser feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Random Forest - Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Logistic Regression\n",
    "print(\"\\n=== LOGISTIC REGRESSION ===\")\n",
    "print(\"\\nBegrundelse:\")\n",
    "print(\"- Simpel og hurtig baseline model\")\n",
    "print(\"- God interpretability (coefficients)\")\n",
    "print(\"- Fungerer godt til lineært separable data\")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"\\n✓ Logistic Regression trænet!\")\n",
    "\n",
    "# Coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Neural Network (MLP)\n",
    "print(\"\\n=== NEURAL NETWORK (MLP) ===\")\n",
    "print(\"\\nBegrundelse:\")\n",
    "print(\"- Kan lære komplekse non-lineære patterns\")\n",
    "print(\"- Hidden layers giver flexibilitet\")\n",
    "print(\"- Moderne ML approach\")\n",
    "\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 10),  # 2 hidden layers med 10 neuroner hver\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,  # Stop når validation performance ikke forbedres\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "print(\"\\n✓ Neural Network trænet!\")\n",
    "print(f\"Antal iterations: {mlp_model.n_iter_}\")\n",
    "print(f\"Final loss: {mlp_model.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Evaluating performance on the test set\n",
    "\n",
    "Vi evaluerer alle 3 modeller på test sættet med:\n",
    "- Confusion Matrix\n",
    "- Precision & Recall\n",
    "- Accuracy\n",
    "- Fejlanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forudsigelser på test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "# Funktion til at vise model performance\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Udtræk værdier\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nTrue Negatives (TN): {tn}  - Korrekt forudsagt 'død'\")\n",
    "    print(f\"False Positives (FP): {fp}  - Forkert forudsagt 'overlevede'\")\n",
    "    print(f\"False Negatives (FN): {fn}  - Forkert forudsagt 'død'\")\n",
    "    print(f\"True Positives (TP): {tp}  - Korrekt forudsagt 'overlevede'\")\n",
    "    \n",
    "    # Beregn metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Manuel accuracy beregning\n",
    "    manual_acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(f\"\\nManuel accuracy: ({tp} + {tn}) / {tp + tn + fp + fn} = {manual_acc:.4f}\")\n",
    "    \n",
    "    # Analyse\n",
    "    print(\"\\nAnalyse:\")\n",
    "    if precision > recall:\n",
    "        print(f\"✓ Modellen er bedst til præcision (få false positives)\")\n",
    "    else:\n",
    "        print(f\"✓ Modellen er bedst til recall (få false negatives)\")\n",
    "    \n",
    "    if fp > fn:\n",
    "        print(f\"⚠ Modellen forudsiger for ofte 'overlevede' (høj FP)\")\n",
    "    elif fn > fp:\n",
    "        print(f\"⚠ Modellen forudsiger for ofte 'død' (høj FN)\")\n",
    "    \n",
    "    return cm, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer Random Forest\n",
    "cm_rf, acc_rf, prec_rf, rec_rf = evaluate_model(\"RANDOM FOREST\", y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer Logistic Regression\n",
    "cm_lr, acc_lr, prec_lr, rec_lr = evaluate_model(\"LOGISTIC REGRESSION\", y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluer Neural Network\n",
    "cm_mlp, acc_mlp, prec_mlp, rec_mlp = evaluate_model(\"NEURAL NETWORK (MLP)\", y_test, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Død', 'Overlevede'], yticklabels=['Død', 'Overlevede'])\n",
    "axes[0].set_title(f'Random Forest\\nAccuracy: {acc_rf:.2%}')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Logistic Regression\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Død', 'Overlevede'], yticklabels=['Død', 'Overlevede'])\n",
    "axes[1].set_title(f'Logistic Regression\\nAccuracy: {acc_lr:.2%}')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "# Neural Network\n",
    "sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "            xticklabels=['Død', 'Overlevede'], yticklabels=['Død', 'Overlevede'])\n",
    "axes[2].set_title(f'Neural Network\\nAccuracy: {acc_mlp:.2%}')\n",
    "axes[2].set_ylabel('Actual')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammenlign alle modeller\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL SAMMENLIGNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Logistic Regression', 'Neural Network'],\n",
    "    'Accuracy': [acc_rf, acc_lr, acc_mlp],\n",
    "    'Precision': [prec_rf, prec_lr, prec_mlp],\n",
    "    'Recall': [rec_rf, rec_lr, rec_mlp]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison)\n",
    "\n",
    "best_model = comparison.loc[comparison['Accuracy'].idxmax(), 'Model']\n",
    "best_acc = comparison['Accuracy'].max()\n",
    "\n",
    "print(f\"\\n✓ Bedste model: {best_model} med {best_acc:.2%} accuracy\")\n",
    "\n",
    "# Visualiser sammenligning\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "comparison.set_index('Model')[['Accuracy', 'Precision', 'Recall']].plot(kind='bar', ax=ax)\n",
    "plt.title('Model Performance Sammenligning')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Experiments\n",
    "\n",
    "Vi eksperimenterer med:\n",
    "1. Random Forest hyperparameters (n_estimators, max_depth)\n",
    "2. Forskellige missing data strategies\n",
    "3. Feature engineering (nye features)\n",
    "4. Neural Network topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperiment 1: Random Forest med forskellige n_estimators\n",
    "print(\"\\n=== EKSPERIMENT 1: Random Forest n_estimators ===\")\n",
    "\n",
    "n_estimators_values = [10, 50, 100, 200, 500]\n",
    "results_exp1 = []\n",
    "\n",
    "for n_est in n_estimators_values:\n",
    "    rf_exp = RandomForestClassifier(n_estimators=n_est, max_depth=10, random_state=42)\n",
    "    rf_exp.fit(X_train_scaled, y_train)\n",
    "    y_pred_exp = rf_exp.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_exp)\n",
    "    results_exp1.append({'n_estimators': n_est, 'accuracy': acc})\n",
    "    print(f\"n_estimators={n_est:3d}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Visualiser\n",
    "exp1_df = pd.DataFrame(results_exp1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(exp1_df['n_estimators'], exp1_df['accuracy'], marker='o')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: n_estimators vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_n = exp1_df.loc[exp1_df['accuracy'].idxmax(), 'n_estimators']\n",
    "print(f\"\\n✓ Bedste n_estimators: {best_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperiment 2: Random Forest med forskellige max_depth\n",
    "print(\"\\n=== EKSPERIMENT 2: Random Forest max_depth ===\")\n",
    "\n",
    "max_depth_values = [3, 5, 10, 15, 20, None]\n",
    "results_exp2 = []\n",
    "\n",
    "for max_d in max_depth_values:\n",
    "    rf_exp = RandomForestClassifier(n_estimators=100, max_depth=max_d, random_state=42)\n",
    "    rf_exp.fit(X_train_scaled, y_train)\n",
    "    y_pred_exp = rf_exp.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_exp)\n",
    "    depth_str = str(max_d) if max_d else 'None'\n",
    "    results_exp2.append({'max_depth': depth_str, 'accuracy': acc})\n",
    "    print(f\"max_depth={depth_str:4s}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "exp2_df = pd.DataFrame(results_exp2)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(exp2_df)), exp2_df['accuracy'], marker='o')\n",
    "plt.xticks(range(len(exp2_df)), exp2_df['max_depth'])\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: max_depth vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperiment 3: Feature Engineering - Family Size\n",
    "print(\"\\n=== EKSPERIMENT 3: Feature Engineering (Family Size) ===\")\n",
    "\n",
    "# Opret ny feature: Family_Size = SibSp + Parch + 1\n",
    "df_exp3 = df_clean.copy()\n",
    "df_exp3['Family_Size'] = df_exp3['SibSp'] + df_exp3['Parch'] + 1\n",
    "\n",
    "print(\"\\nFamily Size distribution:\")\n",
    "print(df_exp3['Family_Size'].value_counts().sort_index())\n",
    "\n",
    "# Træn model med ny feature\n",
    "X_exp3 = df_exp3.drop('Survived', axis=1)\n",
    "y_exp3 = df_exp3['Survived']\n",
    "\n",
    "X_train_exp3, X_test_exp3, y_train_exp3, y_test_exp3 = train_test_split(\n",
    "    X_exp3, y_exp3, test_size=0.2, random_state=42, stratify=y_exp3\n",
    ")\n",
    "\n",
    "scaler_exp3 = StandardScaler()\n",
    "X_train_exp3_scaled = scaler_exp3.fit_transform(X_train_exp3)\n",
    "X_test_exp3_scaled = scaler_exp3.transform(X_test_exp3)\n",
    "\n",
    "rf_exp3 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_exp3.fit(X_train_exp3_scaled, y_train_exp3)\n",
    "y_pred_exp3 = rf_exp3.predict(X_test_exp3_scaled)\n",
    "acc_exp3 = accuracy_score(y_test_exp3, y_pred_exp3)\n",
    "\n",
    "print(f\"\\nOriginal model accuracy: {acc_rf:.4f}\")\n",
    "print(f\"Med Family_Size feature: {acc_exp3:.4f}\")\n",
    "print(f\"Forskel: {acc_exp3 - acc_rf:+.4f}\")\n",
    "\n",
    "if acc_exp3 > acc_rf:\n",
    "    print(\"\\n✓ Family_Size feature forbedrer modellen!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Family_Size feature hjælper ikke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperiment 4: Neural Network topologies\n",
    "print(\"\\n=== EKSPERIMENT 4: Neural Network Topologies ===\")\n",
    "\n",
    "topologies = [\n",
    "    (5,),\n",
    "    (10,),\n",
    "    (5, 5),\n",
    "    (10, 10),\n",
    "    (20, 10),\n",
    "    (10, 10, 10)\n",
    "]\n",
    "\n",
    "results_exp4 = []\n",
    "\n",
    "for topology in topologies:\n",
    "    mlp_exp = MLPClassifier(\n",
    "        hidden_layer_sizes=topology,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    mlp_exp.fit(X_train_scaled, y_train)\n",
    "    y_pred_exp = mlp_exp.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred_exp)\n",
    "    results_exp4.append({'topology': str(topology), 'accuracy': acc})\n",
    "    print(f\"Topology {str(topology):15s}: Accuracy = {acc:.4f}\")\n",
    "\n",
    "exp4_df = pd.DataFrame(results_exp4)\n",
    "best_topology = exp4_df.loc[exp4_df['accuracy'].idxmax(), 'topology']\n",
    "print(f\"\\n✓ Bedste topology: {best_topology}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperiment 5: Missing data strategies\n",
    "print(\"\\n=== EKSPERIMENT 5: Missing Data Strategies ===\")\n",
    "\n",
    "strategies = ['mean', 'median', 'most_frequent']\n",
    "results_exp5 = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    # Reload original data med manglende værdier\n",
    "    df_exp5 = data.copy()\n",
    "    df_exp5 = df_exp5.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    # Encode kategoriske\n",
    "    df_exp5['Sex'] = df_exp5['Sex'].map({'female': 0, 'male': 1})\n",
    "    df_exp5 = pd.get_dummies(df_exp5, columns=['Embarked'], drop_first=True, dtype=int)\n",
    "    \n",
    "    # Split\n",
    "    X_exp5 = df_exp5.drop('Survived', axis=1)\n",
    "    y_exp5 = df_exp5['Survived']\n",
    "    \n",
    "    X_train_exp5, X_test_exp5, y_train_exp5, y_test_exp5 = train_test_split(\n",
    "        X_exp5, y_exp5, test_size=0.2, random_state=42, stratify=y_exp5\n",
    "    )\n",
    "    \n",
    "    # Impute med strategy\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_train_exp5_imp = imputer.fit_transform(X_train_exp5)\n",
    "    X_test_exp5_imp = imputer.transform(X_test_exp5)\n",
    "    \n",
    "    # Scale\n",
    "    scaler_exp5 = StandardScaler()\n",
    "    X_train_exp5_scaled = scaler_exp5.fit_transform(X_train_exp5_imp)\n",
    "    X_test_exp5_scaled = scaler_exp5.transform(X_test_exp5_imp)\n",
    "    \n",
    "    # Train\n",
    "    rf_exp5 = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf_exp5.fit(X_train_exp5_scaled, y_train_exp5)\n",
    "    y_pred_exp5 = rf_exp5.predict(X_test_exp5_scaled)\n",
    "    acc = accuracy_score(y_test_exp5, y_pred_exp5)\n",
    "    \n",
    "    results_exp5.append({'strategy': strategy, 'accuracy': acc})\n",
    "    print(f\"Strategy '{strategy:15s}': Accuracy = {acc:.4f}\")\n",
    "\n",
    "exp5_df = pd.DataFrame(results_exp5)\n",
    "best_strategy = exp5_df.loc[exp5_df['accuracy'].idxmax(), 'strategy']\n",
    "print(f\"\\n✓ Bedste imputation strategy: {best_strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opsummering af Eksperimenter\n",
    "\n",
    "**Eksperiment 1**: Random Forest n_estimators\n",
    "- Testet: 10, 50, 100, 200, 500 træer\n",
    "- Resultat: Performance stabiliserer sig omkring 100 træer\n",
    "\n",
    "**Eksperiment 2**: Random Forest max_depth\n",
    "- Testet: 3, 5, 10, 15, 20, None\n",
    "- Resultat: max_depth=10-15 giver bedst balance (undgår overfitting)\n",
    "\n",
    "**Eksperiment 3**: Feature Engineering (Family_Size)\n",
    "- Oprettede ny feature: SibSp + Parch + 1\n",
    "- Resultat: Kan forbedre performance marginalt\n",
    "\n",
    "**Eksperiment 4**: Neural Network Topologies\n",
    "- Testet forskellige hidden layer konfigurationer\n",
    "- Resultat: (10, 10) eller (20, 10) giver god performance\n",
    "\n",
    "**Eksperiment 5**: Missing Data Strategies\n",
    "- Testet: mean, median, most_frequent\n",
    "- Resultat: median er ofte mest robust (mindre påvirket af outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final sammenligning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL KONKLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nBedste model: {best_model}\")\n",
    "print(f\"Test accuracy: {best_acc:.2%}\")\n",
    "\n",
    "print(\"\\nVigtigste features (fra Random Forest):\")\n",
    "print(feature_importance.head(5))\n",
    "\n",
    "print(\"\\nForbedringer fra eksperimenter:\")\n",
    "print(f\"- Optimal n_estimators: {best_n}\")\n",
    "print(f\"- Optimal imputation: {best_strategy}\")\n",
    "print(f\"- Optimal MLP topology: {best_topology}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
